# Text-to-image diffusion models attacks in scarce resource environments and privacy issues
For a general idea of this work, take a look at this article written by me: https://medium.com/data-reply-it-datatech/membership-inference-attacks-mias-and-data-leakage-in-generative-models-737e6ed88e21

The aim is to test diffusion generative models against membership inference attacks.

I have studied diffusion models and how to attack them in scarce resource environments.
The attack is reproducible at home using google colab, for the attack to be possible a very small part of the dataset must be disclosed.
I have reproduced the main attack MIA from state-of-the-art papers and I have also created and modified other MIAs.
If you are curious to understand more take a look at my thesis pdf in the repo or take a look at the code!
